import logging
import threading
from datetime import datetime, timedelta

from apscheduler.executors.pool import ThreadPoolExecutor
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from dotenv import load_dotenv

from src.ui.scheduler_tab import create_scheduler_tab
from src.utils.deep_research import deep_research
from src.utils.ding_talk import DingTalkRobot
from src.utils.scheduler_utils import SchedulerManager

load_dotenv()
import glob
import asyncio
import argparse
import os

logger = logging.getLogger(__name__)

import gradio as gr

from browser_use.agent.service import Agent
from browser_use.browser.browser import Browser, BrowserConfig
from browser_use.browser.context import (
    BrowserContextWindowSize,
)
from src.utils.agent_state import AgentState, app_state

from src.agent.custom_agent import CustomAgent
from src.browser.custom_browser import CustomBrowser
from src.agent.custom_prompts import CustomSystemPrompt, CustomAgentMessagePrompt
from src.browser.custom_context import BrowserContextConfig
from src.controller.custom_controller import CustomController
from gradio.themes import Citrus, Default, Glass, Monochrome, Ocean, Origin, Soft, Base
from src.utils.utils import update_model_dropdown, get_latest_files, capture_screenshot, MissingAPIKeyError
from src.utils import utils

# Global variables for persistence
_global_browser = None
_global_browser_context = None
_global_agent = None

# Create the global agent state instance
_global_agent_state = AgentState()

# webui config
webui_config_manager = utils.ConfigManager()


def scan_and_register_components(blocks):
    """æ‰«æä¸€ä¸ª Blocks å¯¹è±¡å¹¶æ³¨å†Œå…¶ä¸­çš„æ‰€æœ‰äº¤äº’å¼ç»„ä»¶ï¼Œä½†ä¸åŒ…æ‹¬æŒ‰é’®"""
    global webui_config_manager

    def traverse_blocks(block, prefix=""):
        registered = 0

        # å¤„ç† Blocks è‡ªèº«çš„ç»„ä»¶
        if hasattr(block, "children"):
            for i, child in enumerate(block.children):
                if isinstance(child, gr.components.Component):
                    # æ’é™¤æŒ‰é’® (Button) ç»„ä»¶
                    if getattr(child, "interactive", False) and not isinstance(child, gr.Button):
                        name = f"{prefix}component_{i}"
                        if hasattr(child, "label") and child.label:
                            # ä½¿ç”¨æ ‡ç­¾ä½œä¸ºåç§°çš„ä¸€éƒ¨åˆ†
                            label = child.label
                            name = f"{prefix}{label}"
                        logger.debug(f"Registering component: {name}")
                        webui_config_manager.register_component(name, child)
                        registered += 1
                elif hasattr(child, "children"):
                    # é€’å½’å¤„ç†åµŒå¥—çš„ Blocks
                    new_prefix = f"{prefix}block_{i}_"
                    registered += traverse_blocks(child, new_prefix)

        return registered

    total = traverse_blocks(blocks)
    logger.info(f"Total registered components: {total}")


def save_current_config():
    return webui_config_manager.save_current_config()


def update_ui_from_config(config_file):
    return webui_config_manager.update_ui_from_config(config_file)


def resolve_sensitive_env_variables(text):
    """
    Replace environment variable placeholders ($SENSITIVE_*) with their values.
    Only replaces variables that start with SENSITIVE_.
    """
    if not text:
        return text

    import re

    # Find all $SENSITIVE_* patterns
    env_vars = re.findall(r'\$SENSITIVE_[A-Za-z0-9_]*', text)

    result = text
    for var in env_vars:
        # Remove the $ prefix to get the actual environment variable name
        env_name = var[1:]  # removes the $
        env_value = os.getenv(env_name)
        if env_value is not None:
            # Replace $SENSITIVE_VAR_NAME with its value
            result = result.replace(var, env_value)

    return result


# è°ƒåº¦å™¨å…³é—­é’©å­å‡½æ•°
def shutdown_scheduler():
    """
    åœ¨ç¨‹åºé€€å‡ºæ—¶å…³é—­è°ƒåº¦å™¨
    """
    scheduler = app_state.get_scheduler()
    if scheduler and scheduler.running:
        logger.info("ç¨‹åºé€€å‡ºï¼Œå…³é—­è°ƒåº¦å™¨")
        scheduler.shutdown(wait=False)


# æ³¨å†Œé€€å‡ºé’©å­
import atexit

atexit.register(shutdown_scheduler)


def start_scheduler(llm_provider,
                    llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key, research_task,
                    interval_minutes=60, title_prefix="æ·±åº¦ç ”ç©¶", search_iterations=2, query_per_iter=3):
    """
    å¯åŠ¨å®šæ—¶æ¨é€è°ƒåº¦å™¨

    å‚æ•°:
        research_task: ç ”ç©¶ä»»åŠ¡å†…å®¹
        interval_minutes: æ‰§è¡Œé—´éš”åˆ†é’Ÿæ•°
        title_prefix: æ ‡é¢˜å‰ç¼€
        search_iterations: æœ€å¤§æœç´¢è¿­ä»£æ¬¡æ•°
        query_per_iter: æ¯æ¬¡è¿­ä»£çš„æœ€å¤§æŸ¥è¯¢æ•°é‡

    è¿”å›:
        å¯åŠ¨ç»“æœæ¶ˆæ¯
    """
    try:
        logger.info(f"å°è¯•å¯åŠ¨è°ƒåº¦å™¨ï¼Œä»»åŠ¡: {research_task}, é—´éš”: {interval_minutes} åˆ†é’Ÿ")

        # å‚æ•°éªŒè¯
        if not research_task or not isinstance(research_task, str) or research_task.strip() == "":
            error_msg = "ç ”ç©¶ä»»åŠ¡å¿…é¡»ä¸ºéç©ºå­—ç¬¦ä¸²"
            logger.error(error_msg)
            return error_msg

        if not isinstance(interval_minutes, (int, float)) or interval_minutes < 1:
            error_msg = "æ—¶é—´é—´éš”å¿…é¡»æ˜¯æ­£æ•´æ•°åˆ†é’Ÿæ•°"
            logger.error(error_msg)
            return error_msg

        # ç¡®ä¿é—´éš”æ˜¯æ•´æ•°åˆ†é’Ÿ
        interval_minutes = int(interval_minutes)

        # åˆ›å»ºæ–°è°ƒåº¦å™¨
        new_scheduler = BackgroundScheduler(
            executors={'default': ThreadPoolExecutor(max_workers=20)},
            job_defaults={
                'coalesce': True,  # åˆå¹¶å¤šä¸ªé”™è¿‡çš„ä»»åŠ¡
                'max_instances': 1,  # åŒä¸€ä¸ªä»»åŠ¡æœ€å¤§å¹¶è¡Œæ•°
                'misfire_grace_time': 600  # é”™è¿‡æ‰§è¡Œçš„å®½é™æ—¶é—´
            }
        )

        # åœæ­¢ç°æœ‰è°ƒåº¦å™¨(å¦‚æœå­˜åœ¨)
        current_scheduler = app_state.get_scheduler()
        if current_scheduler:
            try:
                logger.info("å…³é—­ç°æœ‰è°ƒåº¦å™¨")
                current_scheduler.shutdown(wait=False)
            except Exception as e:
                logger.warning(f"å…³é—­æ—§è°ƒåº¦å™¨æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")

        # ç«‹å³æ›´æ–°å…¨å±€è°ƒåº¦å™¨å®ä¾‹ï¼ˆä½¿ç”¨å¼‚æ­¥æ–¹å¼ï¼Œé¿å…é”å†²çªï¼‰
        logger.info("å¼‚æ­¥è®¾ç½®æ–°è°ƒåº¦å™¨ä¸ºå…¨å±€å®ä¾‹")
        app_state.set_scheduler_async(new_scheduler)

        # è®¾ç½®å®šæ—¶è§¦å‘å™¨ - ä½¿ç”¨åˆ†é’Ÿä¸ºå•ä½
        # å°†åˆ†é’Ÿè½¬æ¢ä¸ºæµ®ç‚¹æ•°ä»¥æ”¯æŒç²¾ç¡®è®¡æ—¶
        trigger = IntervalTrigger(minutes=float(interval_minutes))

        # å‡†å¤‡å®šæ—¶ä»»åŠ¡å‚æ•°
        job_args = {
            'research_task': research_task,
            'title_prefix': title_prefix,
            'search_iterations': search_iterations,
            'query_per_iter': query_per_iter
        }

        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡çš„åŒ…è£…å‡½æ•°
        def _scheduled_job_wrapper(**job_kwargs):
            """å¼‚æ­¥å‡½æ•°åŒ…è£…å™¨å‡½æ•°ï¼Œç”¨äºè°ƒåº¦å™¨è°ƒç”¨"""
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
                return loop.run_until_complete(
                    scheduled_job(**job_kwargs)
                )
            finally:
                loop.close()

        # æ·»åŠ ä»»åŠ¡åˆ°è°ƒåº¦å™¨
        new_scheduler.add_job(
            _scheduled_job_wrapper,  # ä½¿ç”¨åŒ…è£…å‡½æ•°
            trigger=trigger,
            kwargs=job_args,
            id='deep_research_job',
            replace_existing=True,
            misfire_grace_time=600
        )

        try:
            # å¯åŠ¨è°ƒåº¦å™¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸æ›´æ–°çŠ¶æ€
            logger.info("å¼€å§‹å¯åŠ¨è°ƒåº¦å™¨...")
            new_scheduler.start()
            logger.info(f"è°ƒåº¦å™¨å·²æˆåŠŸå¯åŠ¨ï¼Œä¸‹æ¬¡æ‰§è¡Œå°†åœ¨ {interval_minutes} åˆ†é’Ÿå")

            # è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´ï¼ˆä»…ç”¨äºç”Ÿæˆæ¶ˆæ¯ï¼‰
            next_run_time = datetime.now() + timedelta(minutes=interval_minutes)

            # å¯åŠ¨åç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡
            logger.info("è°ƒåº¦å™¨å¯åŠ¨åç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡...")

            # åˆ›å»ºä¸€ä¸ªæ–°çº¿ç¨‹æ¥æ‰§è¡Œä»»åŠ¡ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
            def immediate_execute():
                try:
                    # è®¾ç½®æ–°çš„äº‹ä»¶å¾ªç¯
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        # ä½¿ç”¨ç›¸åŒçš„å‚æ•°è¿è¡Œä»»åŠ¡
                        result = loop.run_until_complete(scheduled_job(
                            llm_provider=llm_provider,
                            llm_model_name=llm_model_name,
                            ollama_num_ctx=ollama_num_ctx,
                            llm_temperature=llm_temperature,
                            llm_base_url=llm_base_url,
                            llm_api_key=llm_api_key,
                            research_task=research_task,
                            title_prefix=title_prefix,
                            search_iterations=search_iterations,
                            query_per_iter=query_per_iter,
                            is_manual=True  # æ ‡è®°ä¸ºæ‰‹åŠ¨è§¦å‘
                        ))
                        logger.info(f"ç«‹å³æ‰§è¡Œä»»åŠ¡ç»“æœ: {result}")
                    finally:
                        loop.close()
                except Exception as e:
                    logger.error(f"ç«‹å³æ‰§è¡Œä»»åŠ¡å¼‚å¸¸: {str(e)}", exc_info=True)

            # å¯åŠ¨ç«‹å³æ‰§è¡Œçº¿ç¨‹
            immediate_thread = threading.Thread(target=immediate_execute)
            immediate_thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œä¸é˜»å¡ä¸»ç¨‹åºé€€å‡º
            immediate_thread.start()

            # ç”ŸæˆæˆåŠŸæ¶ˆæ¯
            if interval_minutes >= 60:
                hours = interval_minutes / 60
                if hours == int(hours):
                    # æ•´æ•°å°æ—¶
                    success_msg = f"è°ƒåº¦å™¨å·²æˆåŠŸå¯åŠ¨ï¼Œå°†æ¯ {int(hours)} å°æ—¶æ‰§è¡Œä¸€æ¬¡ã€‚ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')}"
                else:
                    # å°æ•°å°æ—¶
                    success_msg = f"è°ƒåº¦å™¨å·²æˆåŠŸå¯åŠ¨ï¼Œå°†æ¯ {hours:.1f} å°æ—¶æ‰§è¡Œä¸€æ¬¡ã€‚ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                # å°‘äºä¸€å°æ—¶ï¼Œç›´æ¥æ˜¾ç¤ºåˆ†é’Ÿ
                success_msg = f"è°ƒåº¦å™¨å·²æˆåŠŸå¯åŠ¨ï¼Œå°†æ¯ {interval_minutes} åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ã€‚ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')}"

            logger.info(f"è°ƒåº¦å™¨å¯åŠ¨æˆåŠŸ: {success_msg}")
            return success_msg

        except Exception as e:
            error_msg = f"å¯åŠ¨è°ƒåº¦å™¨æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return error_msg

    except Exception as e:
        error_msg = f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return error_msg


# ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¨é€ä»»åŠ¡
async def run_push_task_once(llm_provider,
                             llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key,
                             research_task, title_prefix="æ·±åº¦ç ”ç©¶", search_iterations=2, query_per_iter=3):
    """
    ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¨é€ä»»åŠ¡ (å¼‚æ­¥ç‰ˆæœ¬)

    å‚æ•°:
        research_task: ç ”ç©¶ä»»åŠ¡å†…å®¹
        title_prefix: æ ‡é¢˜å‰ç¼€
        search_iterations: æœ€å¤§æœç´¢è¿­ä»£æ¬¡æ•°
        query_per_iter: æ¯æ¬¡è¿­ä»£çš„æœ€å¤§æŸ¥è¯¢æ•°é‡

    è¿”å›:
        æ‰§è¡Œç»“æœæ¶ˆæ¯, çŠ¶æ€æ–‡æœ¬, ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´æ–‡æœ¬
    """
    try:
        logger.info(f"ç”¨æˆ·è§¦å‘ç«‹å³æ‰§è¡Œä¸€æ¬¡æŒ‰é’®, ç ”ç©¶ä»»åŠ¡: {research_task}")

        # è°ƒç”¨å¼‚æ­¥çš„scheduled_jobå‡½æ•°ï¼ŒæŒ‡å®šä¸ºæ‰‹åŠ¨æ‰§è¡Œæ¨¡å¼
        result, status_text, next_run_text = await scheduled_job(
            llm_provider,
            llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key,
            research_task,
            title_prefix,
            search_iterations,
            query_per_iter,
            is_manual=True
        )

        # è¿”å›ç»“æœ
        return f"æ‰§è¡ŒæˆåŠŸ: {result}", status_text, next_run_text

    except Exception as e:
        error_msg = f"æ‰§è¡Œå¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)

        # æ›´æ–°é”™è¯¯çŠ¶æ€
        app_state.update_scheduler_status(
            {"last_status": f"æ‰§è¡Œå¼‚å¸¸: {str(e)[:50]}", "last_run_time": datetime.now()},
            blocking=False
        )

        # è¿”å›é”™è¯¯çŠ¶æ€
        return error_msg, f"æ‰§è¡ŒçŠ¶æ€: é”™è¯¯ ({str(e)[:50]})", "æ‰§è¡Œå¤±è´¥"


async def execute_push_task(llm_provider,
                            llm_model_name, llm_num_ctx, llm_temperature, llm_base_url, llm_api_key, research_task,
                            title_prefix="æ·±åº¦ç ”ç©¶", search_iterations=2, query_per_iter=3):
    """
    æ‰§è¡Œæ·±åº¦ç ”ç©¶å¹¶å°†ç»“æœæ¨é€åˆ°é’‰é’‰

    å‚æ•°:
        research_task: ç ”ç©¶ä»»åŠ¡å†…å®¹
        title_prefix: æ ‡é¢˜å‰ç¼€
        search_iterations: æœ€å¤§æœç´¢è¿­ä»£æ¬¡æ•°
        query_per_iter: æ¯æ¬¡è¿­ä»£çš„æœ€å¤§æŸ¥è¯¢æ•°é‡

    è¿”å›:
        str: ç»“æœæ¶ˆæ¯
    """
    try:
        start_time = datetime.now()
        logger.info(f"æ·±åº¦ç ”ç©¶ä»»åŠ¡å¼€å§‹æ‰§è¡Œ: {research_task}")

        agent_state = AgentState()
        app_state.clear_stop()

        # è·å–LLMæ¨¡å‹ - ä½¿ç”¨é˜¿é‡Œäº‘é…ç½®
        llm = utils.get_llm_model(
            provider=llm_provider,
            model_name=llm_model_name,
            num_ctx=llm_num_ctx,
            temperature=llm_temperature,
            base_url=llm_base_url,
            api_key=llm_api_key,
        )

        # æ‰§è¡Œæ·±åº¦ç ”ç©¶
        logger.info(f"å¼€å§‹æ‰§è¡Œæ·±åº¦ç ”ç©¶: {research_task}")
        markdown_content, file_path = await deep_research(
            research_task,
            llm,
            agent_state,
            max_search_iterations=search_iterations,
            max_query_num=query_per_iter,
            use_vision=True,
            headless=True,
            use_own_browser=False
        )

        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        duration_str = f"{duration:.2f}ç§’" if duration < 60 else f"{duration / 60:.2f}åˆ†é’Ÿ"

        # å‡†å¤‡æ¨é€å†…å®¹
        if markdown_content:
            # æ ‡é¢˜åŒ…å«ä»»åŠ¡ç®€è¿°å’Œæ‰§è¡Œæ—¶é—´
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            title = f"{title_prefix}: {research_task[:20]}..." if len(
                research_task) > 20 else f"{title_prefix}: {research_task}"

            # æ¶ˆæ¯å¤´éƒ¨æ·»åŠ æ‰§è¡Œä¿¡æ¯
            message_header = f"## {title}\n\n"
            message_header += f"**æ‰§è¡Œæ—¶é—´**: {current_time}\n\n"
            message_header += f"**è€—æ—¶**: {duration_str}\n\n"
            message_header += f"**ä»»åŠ¡**: {research_task}\n\n"
            message_header += "---\n\n"

            # æ‹¼æ¥å†…å®¹ï¼Œé’‰é’‰é™åˆ¶æ¶ˆæ¯é•¿åº¦ï¼Œè¶…é•¿æ—¶éœ€è¦æˆªæ–­
            message = message_header + markdown_content
            if len(message) > 20000:  # é’‰é’‰æ¶ˆæ¯é•¿åº¦é™åˆ¶
                message = message[:19700] + "\n\n...(å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶)"

            # å‘é€åˆ°é’‰é’‰
            logger.info(f"å‘é€ç ”ç©¶ç»“æœåˆ°é’‰é’‰ï¼Œæ ‡é¢˜: {title}, å†…å®¹é•¿åº¦: {len(message)}")
            try:
                send_result = DingTalkRobot.send_markdown(title, message)
                if send_result:
                    result_msg = f"ä»»åŠ¡æ‰§è¡ŒæˆåŠŸå¹¶æ¨é€è‡³é’‰é’‰ï¼Œè€—æ—¶: {duration_str}"
                    logger.info(result_msg)
                    app_state.update_scheduler_status({"last_status": "æ‰§è¡ŒæˆåŠŸ"}, blocking=False)
                else:
                    result_msg = f"ä»»åŠ¡æ‰§è¡ŒæˆåŠŸä½†é’‰é’‰æ¨é€å¤±è´¥ï¼Œè€—æ—¶: {duration_str}"
                    logger.error(result_msg)
                    app_state.update_scheduler_status({"last_status": "æ¨é€å¤±è´¥"}, blocking=False)
            except Exception as e:
                result_msg = f"é’‰é’‰æ¨é€å¤±è´¥: {str(e)}"
                logger.error(result_msg, exc_info=True)
                app_state.update_scheduler_status({"last_status": "æ¨é€å¼‚å¸¸"}, blocking=False)
        else:
            result_msg = "ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œæœªè·å¾—ç ”ç©¶ç»“æœ"
            logger.warning(result_msg)
            app_state.update_scheduler_status({"last_status": "æ‰§è¡Œå¤±è´¥"}, blocking=False)

        return result_msg, markdown_content, file_path

    except Exception as e:
        error_msg = f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}"
        logger.error(error_msg, exc_info=True)
        app_state.update_scheduler_status({"last_status": f"æ‰§è¡Œå¼‚å¸¸: {str(e)[:50]}"})

        # å‘é€é”™è¯¯é€šçŸ¥åˆ°é’‰é’‰
        try:
            error_title = "æ·±åº¦ç ”ç©¶ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸"
            error_content = f"## æ·±åº¦ç ”ç©¶ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸\n\n"
            error_content += f"**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            error_content += f"**ä»»åŠ¡**: {research_task}\n\n"
            error_content += f"**é”™è¯¯**: {str(e)}\n\n"
            DingTalkRobot.send_markdown(error_title, error_content)
        except Exception as send_error:
            logger.error(f"å‘é€é”™è¯¯é€šçŸ¥å¤±è´¥: {str(send_error)}")

        return error_msg, None, None


async def scheduled_job(llm_provider,
                        llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key, research_task,
                        title_prefix="æ·±åº¦æœç´¢", search_iterations=2, query_per_iter=3, is_manual=False):
    """
    å®šæ—¶ä»»åŠ¡å…¥å£å‡½æ•° - å¼‚æ­¥ç‰ˆæœ¬

    å‚æ•°:
        research_task: ç ”ç©¶ä»»åŠ¡
        title_prefix: æ ‡é¢˜å‰ç¼€
        search_iterations: æœç´¢è¿­ä»£æ¬¡æ•°
        query_per_iter: æ¯æ¬¡è¿­ä»£æŸ¥è¯¢æ•°é‡
        is_manual: æ˜¯å¦ä¸ºæ‰‹åŠ¨è§¦å‘çš„ä»»åŠ¡

    è¿”å›:
        æ‰§è¡Œç»“æœã€çŠ¶æ€æ–‡æœ¬ã€ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´æ–‡æœ¬
    """
    try:
        if not research_task or not isinstance(research_task, str) or research_task.strip() == "":
            raise ValueError("ç ”ç©¶ä»»åŠ¡ä¸èƒ½ä¸ºç©º")

        mode = "æ‰‹åŠ¨" if is_manual else "å®šæ—¶"
        logger.info(f"{mode}ä»»åŠ¡è§¦å‘ï¼Œç ”ç©¶ä»»åŠ¡: {research_task}")

        # è·å–ä»£ç†çŠ¶æ€
        app_state.get_agent_state()
        # ä½¿ç”¨app_stateæ›¿ä»£agent_stateè°ƒç”¨clear_stopæ–¹æ³•
        app_state.clear_stop()

        # è·å–è¶…æ—¶è®¾ç½®
        timeout_seconds = int(os.getenv("TASK_TIMEOUT", "600"))

        # ç›´æ¥å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
        try:
            # ä½¿ç”¨asyncio.wait_forè®¾ç½®è¶…æ—¶
            result, markdown_content, file_path = await asyncio.wait_for(
                execute_push_task(
                    llm_provider,
                    llm_model_name,
                    ollama_num_ctx,
                    llm_temperature,
                    llm_base_url,
                    llm_api_key,
                    research_task,
                    title_prefix,
                    search_iterations,
                    query_per_iter
                ),
                timeout=timeout_seconds
            )

            # ä»»åŠ¡æˆåŠŸå®Œæˆ
            logger.info(f"{mode}ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {research_task}")

            # æ›´æ–°æœ€ç»ˆçŠ¶æ€ï¼ˆä¸ä½¿ç”¨ä¸­é—´çŠ¶æ€ï¼‰
            app_state.update_scheduler_status(
                {"last_status": f"{mode}æ‰§è¡Œå®Œæˆ", "last_run_time": datetime.now()},
                blocking=False
            )

            # è¿”å›æˆåŠŸç»“æœ
            status_text = f"æ‰§è¡ŒçŠ¶æ€: æˆåŠŸå®Œæˆ"
            next_run_text = "æ‰‹åŠ¨æ‰§è¡Œå®Œæˆ"
            if not is_manual:
                next_run = get_next_run_time()
                if next_run:
                    next_run_text = f"ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {next_run.strftime('%Y-%m-%d %H:%M:%S')}"

            return result, status_text, next_run_text

        except asyncio.TimeoutError:
            # å¤„ç†ä»»åŠ¡è¶…æ—¶
            error_msg = f"ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰"
            logger.error(error_msg)

            # æ›´æ–°è¶…æ—¶çŠ¶æ€
            app_state.update_scheduler_status(
                {"last_status": "æ‰§è¡Œè¶…æ—¶", "last_run_time": datetime.now()},
                blocking=False
            )

            # å°è¯•å‘é€è¶…æ—¶é€šçŸ¥
            try:
                timeout_title = f"ã€{title_prefix}ã€‘ä»»åŠ¡æ‰§è¡Œè¶…æ—¶"
                timeout_text = f"ç ”ç©¶ä»»åŠ¡ '{research_task}' æ‰§è¡Œè¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰"
                # å¦‚æœéœ€è¦å‘é€é€šçŸ¥ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ä»£ç 
            except Exception as e:
                logger.error(f"å‘é€è¶…æ—¶é€šçŸ¥å¤±è´¥ï¼š{str(e)}")

            return f"æ‰§è¡Œè¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰", "æ‰§è¡ŒçŠ¶æ€: è¶…æ—¶", "æ‰§è¡Œå¤±è´¥"

    except Exception as e:
        # å¤„ç†å…¶ä»–å¼‚å¸¸
        error_msg = f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}"
        logger.error(error_msg, exc_info=True)

        # æ›´æ–°å¼‚å¸¸çŠ¶æ€
        app_state.update_scheduler_status(
            {"last_status": f"æ‰§è¡Œå¼‚å¸¸: {str(e)[:50]}", "last_run_time": datetime.now()},
            blocking=False
        )

        return f"ä»»åŠ¡å¼‚å¸¸: {str(e)}", f"æ‰§è¡ŒçŠ¶æ€: å¼‚å¸¸ ({str(e)[:50]})", "æ‰§è¡Œå¤±è´¥"


async def stop_agent():
    """Request the agent to stop and update UI with enhanced feedback"""
    global _global_agent

    try:
        if _global_agent is not None:
            # Request stop
            _global_agent.stop()
        # Update UI immediately
        message = "Stop requested - the agent will halt at the next safe point"
        logger.info(f"ğŸ›‘ {message}")

        # Return UI updates
        return (
            gr.update(value="Stopping...", interactive=False),  # stop_button
            gr.update(interactive=False),  # run_button
        )
    except Exception as e:
        error_msg = f"Error during stop: {str(e)}"
        logger.error(error_msg)
        return (
            gr.update(value="Stop", interactive=True),
            gr.update(interactive=True)
        )


async def stop_research_agent():
    """Request the agent to stop and update UI with enhanced feedback"""
    global _global_agent_state

    try:
        # Request stop
        _global_agent_state.request_stop()

        # Update UI immediately
        message = "Stop requested - the agent will halt at the next safe point"
        logger.info(f"ğŸ›‘ {message}")

        # Return UI updates
        return (  # errors_output
            gr.update(value="Stopping...", interactive=False),  # stop_button
            gr.update(interactive=False),  # run_button
        )
    except Exception as e:
        error_msg = f"Error during stop: {str(e)}"
        logger.error(error_msg)
        return (
            gr.update(value="Stop", interactive=True),
            gr.update(interactive=True)
        )


async def run_browser_agent(
        agent_type,
        llm_provider,
        llm_model_name,
        llm_num_ctx,
        llm_temperature,
        llm_base_url,
        llm_api_key,
        use_own_browser,
        keep_browser_open,
        headless,
        disable_security,
        window_w,
        window_h,
        save_recording_path,
        save_agent_history_path,
        save_trace_path,
        enable_recording,
        task,
        add_infos,
        max_steps,
        use_vision,
        max_actions_per_step,
        tool_calling_method,
        chrome_cdp,
        max_input_tokens
):
    try:
        # Disable recording if the checkbox is unchecked
        if not enable_recording:
            save_recording_path = None

        # Ensure the recording directory exists if recording is enabled
        if save_recording_path:
            os.makedirs(save_recording_path, exist_ok=True)

        # Get the list of existing videos before the agent runs
        existing_videos = set()
        if save_recording_path:
            existing_videos = set(
                glob.glob(os.path.join(save_recording_path, "*.[mM][pP]4"))
                + glob.glob(os.path.join(save_recording_path, "*.[wW][eE][bB][mM]"))
            )

        task = resolve_sensitive_env_variables(task)

        # Run the agent
        llm = utils.get_llm_model(
            provider=llm_provider,
            model_name=llm_model_name,
            num_ctx=llm_num_ctx,
            temperature=llm_temperature,
            base_url=llm_base_url,
            api_key=llm_api_key,
        )
        if agent_type == "org":
            final_result, errors, model_actions, model_thoughts, trace_file, history_file = await run_org_agent(
                llm=llm,
                use_own_browser=use_own_browser,
                keep_browser_open=keep_browser_open,
                headless=headless,
                disable_security=disable_security,
                window_w=window_w,
                window_h=window_h,
                save_recording_path=save_recording_path,
                save_agent_history_path=save_agent_history_path,
                save_trace_path=save_trace_path,
                task=task,
                max_steps=max_steps,
                use_vision=use_vision,
                max_actions_per_step=max_actions_per_step,
                tool_calling_method=tool_calling_method,
                chrome_cdp=chrome_cdp,
                max_input_tokens=max_input_tokens
            )
        elif agent_type == "custom":
            final_result, errors, model_actions, model_thoughts, trace_file, history_file = await run_custom_agent(
                llm=llm,
                use_own_browser=use_own_browser,
                keep_browser_open=keep_browser_open,
                headless=headless,
                disable_security=disable_security,
                window_w=window_w,
                window_h=window_h,
                save_recording_path=save_recording_path,
                save_agent_history_path=save_agent_history_path,
                save_trace_path=save_trace_path,
                task=task,
                add_infos=add_infos,
                max_steps=max_steps,
                use_vision=use_vision,
                max_actions_per_step=max_actions_per_step,
                tool_calling_method=tool_calling_method,
                chrome_cdp=chrome_cdp,
                max_input_tokens=max_input_tokens
            )
        else:
            raise ValueError(f"Invalid agent type: {agent_type}")

        # Get the list of videos after the agent runs (if recording is enabled)
        # latest_video = None
        # if save_recording_path:
        #     new_videos = set(
        #         glob.glob(os.path.join(save_recording_path, "*.[mM][pP]4"))
        #         + glob.glob(os.path.join(save_recording_path, "*.[wW][eE][bB][mM]"))
        #     )
        #     if new_videos - existing_videos:
        #         latest_video = list(new_videos - existing_videos)[0]  # Get the first new video

        gif_path = os.path.join(os.path.dirname(__file__), "agent_history.gif")

        return (
            final_result,
            errors,
            model_actions,
            model_thoughts,
            gif_path,
            trace_file,
            history_file,
            gr.update(value="Stop", interactive=True),  # Re-enable stop button
            gr.update(interactive=True)  # Re-enable run button
        )

    except MissingAPIKeyError as e:
        logger.error(str(e))
        raise gr.Error(str(e), print_exception=False)

    except Exception as e:
        import traceback
        traceback.print_exc()
        errors = str(e) + "\n" + traceback.format_exc()
        return (
            '',  # final_result
            errors,  # errors
            '',  # model_actions
            '',  # model_thoughts
            None,  # latest_video
            None,  # history_file
            None,  # trace_file
            gr.update(value="Stop", interactive=True),  # Re-enable stop button
            gr.update(interactive=True)  # Re-enable run button
        )


async def run_org_agent(
        llm,
        use_own_browser,
        keep_browser_open,
        headless,
        disable_security,
        window_w,
        window_h,
        save_recording_path,
        save_agent_history_path,
        save_trace_path,
        task,
        max_steps,
        use_vision,
        max_actions_per_step,
        tool_calling_method,
        chrome_cdp,
        max_input_tokens
):
    try:
        global _global_browser, _global_browser_context, _global_agent

        extra_chromium_args = ["--accept_downloads=True", f"--window-size={window_w},{window_h}"]
        cdp_url = chrome_cdp

        if use_own_browser:
            cdp_url = os.getenv("CHROME_CDP", chrome_cdp)
            chrome_path = os.getenv("CHROME_PATH", None)
            if chrome_path == "":
                chrome_path = None
            chrome_user_data = os.getenv("CHROME_USER_DATA", None)
            if chrome_user_data:
                extra_chromium_args += [f"--user-data-dir={chrome_user_data}"]
        else:
            chrome_path = None

        if _global_browser is None:
            _global_browser = Browser(
                config=BrowserConfig(
                    headless=headless,
                    cdp_url=cdp_url,
                    disable_security=disable_security,
                    chrome_instance_path=chrome_path,
                    extra_chromium_args=extra_chromium_args,
                )
            )

        if _global_browser_context is None:
            _global_browser_context = await _global_browser.new_context(
                config=BrowserContextConfig(
                    trace_path=save_trace_path if save_trace_path else None,
                    save_recording_path=save_recording_path if save_recording_path else None,
                    save_downloads_path="./tmp/downloads",
                    no_viewport=False,
                    browser_window_size=BrowserContextWindowSize(
                        width=window_w, height=window_h
                    ),
                )
            )

        if _global_agent is None:
            _global_agent = Agent(
                task=task,
                llm=llm,
                use_vision=use_vision,
                browser=_global_browser,
                browser_context=_global_browser_context,
                max_actions_per_step=max_actions_per_step,
                tool_calling_method=tool_calling_method,
                max_input_tokens=max_input_tokens,
                generate_gif=True
            )
        history = await _global_agent.run(max_steps=max_steps)

        history_file = os.path.join(save_agent_history_path, f"{_global_agent.state.agent_id}.json")
        _global_agent.save_history(history_file)

        final_result = history.final_result()
        errors = history.errors()
        model_actions = history.model_actions()
        model_thoughts = history.model_thoughts()

        trace_file = get_latest_files(save_trace_path)

        return final_result, errors, model_actions, model_thoughts, trace_file.get('.zip'), history_file
    except Exception as e:
        import traceback
        traceback.print_exc()
        errors = str(e) + "\n" + traceback.format_exc()
        return '', errors, '', '', None, None
    finally:
        _global_agent = None
        # Handle cleanup based on persistence configuration
        if not keep_browser_open:
            if _global_browser_context:
                await _global_browser_context.close()
                _global_browser_context = None

            if _global_browser:
                await _global_browser.close()
                _global_browser = None


async def run_custom_agent(
        llm,
        use_own_browser,
        keep_browser_open,
        headless,
        disable_security,
        window_w,
        window_h,
        save_recording_path,
        save_agent_history_path,
        save_trace_path,
        task,
        add_infos,
        max_steps,
        use_vision,
        max_actions_per_step,
        tool_calling_method,
        chrome_cdp,
        max_input_tokens
):
    try:
        global _global_browser, _global_browser_context, _global_agent

        extra_chromium_args = ["--accept_downloads=True", f"--window-size={window_w},{window_h}"]
        cdp_url = chrome_cdp
        if use_own_browser:
            cdp_url = os.getenv("CHROME_CDP", chrome_cdp)

            chrome_path = os.getenv("CHROME_PATH", None)
            if chrome_path == "":
                chrome_path = None
            chrome_user_data = os.getenv("CHROME_USER_DATA", None)
            if chrome_user_data:
                extra_chromium_args += [f"--user-data-dir={chrome_user_data}"]
        else:
            chrome_path = None

        controller = CustomController()

        # Initialize global browser if needed
        # if chrome_cdp not empty string nor None
        if (_global_browser is None) or (cdp_url and cdp_url != "" and cdp_url != None):
            _global_browser = CustomBrowser(
                config=BrowserConfig(
                    headless=headless,
                    disable_security=disable_security,
                    cdp_url=cdp_url,
                    chrome_instance_path=chrome_path,
                    extra_chromium_args=extra_chromium_args,
                )
            )

        if _global_browser_context is None or (chrome_cdp and cdp_url != "" and cdp_url != None):
            _global_browser_context = await _global_browser.new_context(
                config=BrowserContextConfig(
                    trace_path=save_trace_path if save_trace_path else None,
                    save_recording_path=save_recording_path if save_recording_path else None,
                    no_viewport=False,
                    save_downloads_path="./tmp/downloads",
                    browser_window_size=BrowserContextWindowSize(
                        width=window_w, height=window_h
                    ),
                )
            )

        # Create and run agent
        if _global_agent is None:
            _global_agent = CustomAgent(
                task=task,
                add_infos=add_infos,
                use_vision=use_vision,
                llm=llm,
                browser=_global_browser,
                browser_context=_global_browser_context,
                controller=controller,
                system_prompt_class=CustomSystemPrompt,
                agent_prompt_class=CustomAgentMessagePrompt,
                max_actions_per_step=max_actions_per_step,
                tool_calling_method=tool_calling_method,
                max_input_tokens=max_input_tokens,
                generate_gif=True
            )
        history = await _global_agent.run(max_steps=max_steps)

        history_file = os.path.join(save_agent_history_path, f"{_global_agent.state.agent_id}.json")
        _global_agent.save_history(history_file)

        final_result = history.final_result()
        errors = history.errors()
        model_actions = history.model_actions()
        model_thoughts = history.model_thoughts()

        trace_file = get_latest_files(save_trace_path)

        return final_result, errors, model_actions, model_thoughts, trace_file.get('.zip'), history_file
    except Exception as e:
        import traceback
        traceback.print_exc()
        errors = str(e) + "\n" + traceback.format_exc()
        return '', errors, '', '', None, None
    finally:
        _global_agent = None
        # Handle cleanup based on persistence configuration
        if not keep_browser_open:
            if _global_browser_context:
                await _global_browser_context.close()
                _global_browser_context = None

            if _global_browser:
                await _global_browser.close()
                _global_browser = None


async def run_with_stream(
        agent_type,
        llm_provider,
        llm_model_name,
        llm_num_ctx,
        llm_temperature,
        llm_base_url,
        llm_api_key,
        use_own_browser,
        keep_browser_open,
        headless,
        disable_security,
        window_w,
        window_h,
        save_recording_path,
        save_agent_history_path,
        save_trace_path,
        enable_recording,
        task,
        add_infos,
        max_steps,
        use_vision,
        max_actions_per_step,
        tool_calling_method,
        chrome_cdp,
        max_input_tokens
):
    global _global_agent

    stream_vw = 80
    stream_vh = int(80 * window_h // window_w)
    if not headless:
        result = await run_browser_agent(
            agent_type=agent_type,
            llm_provider=llm_provider,
            llm_model_name=llm_model_name,
            llm_num_ctx=llm_num_ctx,
            llm_temperature=llm_temperature,
            llm_base_url=llm_base_url,
            llm_api_key=llm_api_key,
            use_own_browser=use_own_browser,
            keep_browser_open=keep_browser_open,
            headless=headless,
            disable_security=disable_security,
            window_w=window_w,
            window_h=window_h,
            save_recording_path=save_recording_path,
            save_agent_history_path=save_agent_history_path,
            save_trace_path=save_trace_path,
            enable_recording=enable_recording,
            task=task,
            add_infos=add_infos,
            max_steps=max_steps,
            use_vision=use_vision,
            max_actions_per_step=max_actions_per_step,
            tool_calling_method=tool_calling_method,
            chrome_cdp=chrome_cdp,
            max_input_tokens=max_input_tokens
        )
        # Add HTML content at the start of the result array
        yield [gr.update(visible=False)] + list(result)
    else:
        try:
            # Run the browser agent in the background
            agent_task = asyncio.create_task(
                run_browser_agent(
                    agent_type=agent_type,
                    llm_provider=llm_provider,
                    llm_model_name=llm_model_name,
                    llm_num_ctx=llm_num_ctx,
                    llm_temperature=llm_temperature,
                    llm_base_url=llm_base_url,
                    llm_api_key=llm_api_key,
                    use_own_browser=use_own_browser,
                    keep_browser_open=keep_browser_open,
                    headless=headless,
                    disable_security=disable_security,
                    window_w=window_w,
                    window_h=window_h,
                    save_recording_path=save_recording_path,
                    save_agent_history_path=save_agent_history_path,
                    save_trace_path=save_trace_path,
                    enable_recording=enable_recording,
                    task=task,
                    add_infos=add_infos,
                    max_steps=max_steps,
                    use_vision=use_vision,
                    max_actions_per_step=max_actions_per_step,
                    tool_calling_method=tool_calling_method,
                    chrome_cdp=chrome_cdp,
                    max_input_tokens=max_input_tokens
                )
            )

            # Initialize values for streaming
            html_content = f"<h1 style='width:{stream_vw}vw; height:{stream_vh}vh'>Using browser...</h1>"
            final_result = errors = model_actions = model_thoughts = ""
            recording_gif = trace = history_file = None

            # Periodically update the stream while the agent task is running
            while not agent_task.done():
                try:
                    encoded_screenshot = await capture_screenshot(_global_browser_context)
                    if encoded_screenshot is not None:
                        html_content = f'<img src="data:image/jpeg;base64,{encoded_screenshot}" style="width:{stream_vw}vw; height:{stream_vh}vh ; border:1px solid #ccc;">'
                    else:
                        html_content = f"<h1 style='width:{stream_vw}vw; height:{stream_vh}vh'>Waiting for browser session...</h1>"
                except Exception as e:
                    html_content = f"<h1 style='width:{stream_vw}vw; height:{stream_vh}vh'>Waiting for browser session...</h1>"

                if _global_agent and _global_agent.state.stopped:
                    yield [
                        gr.HTML(value=html_content, visible=True),
                        final_result,
                        errors,
                        model_actions,
                        model_thoughts,
                        recording_gif,
                        trace,
                        history_file,
                        gr.update(value="Stopping...", interactive=False),  # stop_button
                        gr.update(interactive=False),  # run_button
                    ]
                    break
                else:
                    yield [
                        gr.HTML(value=html_content, visible=True),
                        final_result,
                        errors,
                        model_actions,
                        model_thoughts,
                        recording_gif,
                        trace,
                        history_file,
                        gr.update(),  # Re-enable stop button
                        gr.update()  # Re-enable run button
                    ]
                await asyncio.sleep(0.1)

            # Once the agent task completes, get the results
            try:
                result = await agent_task
                final_result, errors, model_actions, model_thoughts, recording_gif, trace, history_file, stop_button, run_button = result
            except gr.Error:
                final_result = ""
                model_actions = ""
                model_thoughts = ""
                recording_gif = trace = history_file = None

            except Exception as e:
                errors = f"Agent error: {str(e)}"

            yield [
                gr.HTML(value=html_content, visible=True),
                final_result,
                errors,
                model_actions,
                model_thoughts,
                recording_gif,
                trace,
                history_file,
                stop_button,
                run_button
            ]

        except Exception as e:
            import traceback
            yield [
                gr.HTML(
                    value=f"<h1 style='width:{stream_vw}vw; height:{stream_vh}vh'>Waiting for browser session...</h1>",
                    visible=True),
                "",
                f"Error: {str(e)}\n{traceback.format_exc()}",
                "",
                "",
                None,
                None,
                None,
                gr.update(value="Stop", interactive=True),  # Re-enable stop button
                gr.update(interactive=True)  # Re-enable run button
            ]


# Define the theme map globally
theme_map = {
    "Default": Default(),
    "Soft": Soft(),
    "Monochrome": Monochrome(),
    "Glass": Glass(),
    "Origin": Origin(),
    "Citrus": Citrus(),
    "Ocean": Ocean(),
    "Base": Base()
}


async def close_global_browser():
    global _global_browser, _global_browser_context

    if _global_browser_context:
        await _global_browser_context.close()
        _global_browser_context = None

    if _global_browser:
        await _global_browser.close()
        _global_browser = None


# Function to optimize the research task prompt
async def optimize_prompt(prompt_text, llm_provider,
                          llm_model_name, llm_num_ctx, llm_temperature, llm_base_url, llm_api_key):
    try:
        llm = utils.get_llm_model(
            provider=llm_provider,
            model_name=llm_model_name,
            num_ctx=llm_num_ctx,
            temperature=llm_temperature,
            base_url=llm_base_url,
            api_key=llm_api_key,
        )
        prompt = '''ä½ æ˜¯ä¸€ä¸ªç ”ç©¶ä»»åŠ¡æ‰©å±•ä¸“å®¶ã€‚å½“ç”¨æˆ·æå‡ºç®€çŸ­çš„ç ”ç©¶éœ€æ±‚æ—¶ï¼Œè¯·ä½ ä»ç”¨æˆ·çš„è§’åº¦å‡ºå‘ï¼Œå°†å…¶æ‰©å±•ä¸ºæ›´å…¨é¢ã€æ›´æœ‰æ·±åº¦çš„ç ”ç©¶è¯·æ±‚ã€‚å…·ä½“æ¥è¯´ï¼š
        1.è¯†åˆ«ç”¨æˆ·ç ”ç©¶ä¸»é¢˜çš„æ ¸å¿ƒé¢†åŸŸ
        2.è¡¥å……3 - 5ä¸ªå¯èƒ½çš„ç ”ç©¶è§’åº¦æˆ–ç»´åº¦
        3.æå‡º2 - 3ä¸ªå¯èƒ½çš„åˆ†ææ–¹æ³•æˆ–æ­¥éª¤
        4.æ˜ç¡®1 - 2ä¸ªæœŸæœ›çš„ç ”ç©¶æˆæœæˆ–åº”ç”¨

        ä¿æŒç”¨æˆ·çš„ç¬¬ä¸€äººç§°è¯­æ°”ï¼Œç›´æ¥è¿”å›æ‰©å±•åçš„ç ”ç©¶éœ€æ±‚ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚ä¾‹å¦‚ï¼Œå½“ç”¨æˆ·è¾“å…¥'æˆ‘è¦ä¸€ä»½ç‰›å¥¶åˆ†ææŠ¥å‘Š'æ—¶ï¼Œä½ åº”è¯¥å°†å…¶æ‰©å±•ä¸º'æˆ‘éœ€è¦ä¸€ä»½å…¨é¢çš„ç‰›å¥¶åˆ†ææŠ¥å‘Šï¼Œå¸Œæœ›ä»å¸‚åœºè¶‹åŠ¿ã€è¥å…»æˆåˆ†å¯¹æ¯”ã€æ¶ˆè´¹è€…åå¥½å’Œä»·æ ¼æ³¢åŠ¨ç­‰è§’åº¦è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚å»ºè®®é‡‡ç”¨æ•°æ®å¯è§†åŒ–æ–¹å¼å‘ˆç°å¸‚åœºä»½é¢å˜åŒ–ï¼Œå¹¶é€šè¿‡å¯¹æ¯”åˆ†ææ­ç¤ºä¸åŒå“ç‰Œçš„ä¼˜åŠ£åŠ¿ã€‚æœ€ç»ˆå¸Œæœ›è¯¥æŠ¥å‘Šèƒ½æŒ‡å¯¼æˆ‘çš„è´­ä¹°å†³ç­–å¹¶æä¾›æœªæ¥ç‰›å¥¶è¡Œä¸šå‘å±•è¶‹åŠ¿çš„æ´å¯Ÿã€‚'

        ç”¨æˆ·è¾“å…¥çš„æ˜¯ï¼š'''
        ai_query_msg = llm.invoke(prompt + prompt_text)
        return ai_query_msg.content
    except Exception as e:
        logger.error(f"Error optimizing prompt: {str(e)}")
        return f"ä¼˜åŒ–å¤±è´¥: {str(e)}"


async def run_deep_search(
        enable_schedule,
        schedule_interval,
        schedule_title,
        research_task, max_search_iteration_input, max_query_per_iter_input, llm_provider,
        llm_model_name, llm_num_ctx, llm_temperature, llm_base_url, llm_api_key, use_vision,
        use_own_browser, headless, chrome_cdp):
    from src.utils.deep_research import deep_research
    from src.utils.ppt_download import get_ppt
    import os
    global _global_agent_state

    # Clear any previous stop request
    # ä½¿ç”¨app_stateæ›¿ä»£agent_stateè°ƒç”¨clear_stopæ–¹æ³•
    app_state.clear_stop()

    llm = utils.get_llm_model(
        provider=llm_provider,
        model_name=llm_model_name,
        num_ctx=llm_num_ctx,
        temperature=llm_temperature,
        base_url=llm_base_url,
        api_key=llm_api_key,
    )

    if enable_schedule:
        scheduler = SchedulerManager()
        # ä½¿ç”¨å…³é”®å­—å‚æ•°ä¼ é€’ï¼Œé¿å…ä½ç½®å‚æ•°é—®é¢˜

        # å®šä¹‰ä¸€ä¸ªå…¨å±€å­˜å‚¨ç»“æœçš„å­—å…¸
        result_store = {}

        # å®šä¹‰å›è°ƒå‡½æ•°æ¥å¤„ç†è¿”å›å€¼
        async def process_result(markdown_content, file_path):
            # å­˜å‚¨ç»“æœ
            result_store['markdown_content'] = markdown_content
            result_store['file_path'] = file_path

            # ç”ŸæˆPPT
            if file_path and os.path.exists(file_path) and markdown_content:
                try:
                    # è°ƒç”¨get_pptå‡½æ•°ç”ŸæˆPPT
                    ppt_path = get_ppt(markdown_content)
                    result_store['ppt_path'] = ppt_path

                    # è®°å½•æ—¥å¿—
                    logger.info(f"è®¡åˆ’ä»»åŠ¡ç”ŸæˆPPTæˆåŠŸ: {ppt_path}")
                except Exception as e:
                    logger.error(f"è®¡åˆ’ä»»åŠ¡ç”ŸæˆPPTå¤±è´¥: {str(e)}")

        # è‡ªå®šä¹‰åŒ…è£…å¼‚æ­¥å‡½æ•°ï¼Œç¡®ä¿å‚æ•°æ­£ç¡®ä¼ é€’
        async def wrapped_deep_research():
            result = await deep_research(
                task=research_task,
                llm=llm,
                agent_state=_global_agent_state,
                max_search_iterations=max_search_iteration_input,
                max_query_num=max_query_per_iter_input,
                use_vision=use_vision,
                headless=headless,
                use_own_browser=use_own_browser,
                chrome_cdp=chrome_cdp
            )
            # å¤„ç†ç»“æœ
            if isinstance(result, tuple) and len(result) >= 2:
                markdown_content, file_path = result[0], result[1]
                await process_result(markdown_content, file_path)
            return result

        # æ·»åŠ åŒ…è£…åçš„å¼‚æ­¥ä»»åŠ¡
        async_job_id = scheduler.add_async_job(
            async_func=wrapped_deep_research,
            trigger='interval',
            minutes=schedule_interval,  # ä½¿ç”¨ä¼ å…¥çš„é—´éš”å‚æ•°ï¼Œè½¬æ¢ä¸ºåˆ†é’Ÿ
            job_id=f"deep_research_{schedule_title}"  # æ·»åŠ æœ‰æ„ä¹‰çš„ä»»åŠ¡ID
        )

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡
        scheduler.execute_job_now(async_job_id)

        # è½®è¯¢ç­‰å¾…result_storeæœ‰å€¼ï¼Œæœ€å¤šç­‰å¾…60ç§’
        wait_time = 0
        max_wait_time = 600  # æœ€å¤§ç­‰å¾…æ—¶é—´ï¼Œç§’
        check_interval = 2   # æ£€æŸ¥é—´éš”ï¼Œç§’

        logger.info(f"å¼€å§‹ç­‰å¾…å¼‚æ­¥ä»»åŠ¡ç»“æœ...")

        # åˆå§‹åŒ–ç»“æœå˜é‡
        markdown_content, file_path = None, None

        # è½®è¯¢ç­‰å¾…ç»“æœ
        while wait_time < max_wait_time:
            # æ£€æŸ¥result_storeæ˜¯å¦æœ‰å€¼
            if 'markdown_content' in result_store and 'file_path' in result_store:
                markdown_content = result_store.get('markdown_content')
                file_path = result_store.get('file_path')
                logger.info(f"æˆåŠŸè·å–å¼‚æ­¥ä»»åŠ¡ç»“æœï¼Œè€—æ—¶: {wait_time}ç§’")
                break

            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
            await asyncio.sleep(check_interval)
            wait_time += check_interval
            logger.info(f"ç­‰å¾…å¼‚æ­¥ä»»åŠ¡ç»“æœ: {wait_time}ç§’...")

        # å¦‚æœè¶…æ—¶ä»æœªè·å–ç»“æœ
        if wait_time >= max_wait_time:
            logger.warning(f"ç­‰å¾…å¼‚æ­¥ä»»åŠ¡ç»“æœè¶…æ—¶({max_wait_time}ç§’)ï¼Œç»§ç»­å¤„ç†")

        # è¿”å›ç»“æœï¼Œå¦‚æœæœªè·å–åˆ°ç»“æœåˆ™ä¸ºNone
        logger.info(f"è¿”å›ç»“æœ: æ–‡ä»¶è·¯å¾„={file_path}, å†…å®¹é•¿åº¦={len(markdown_content) if markdown_content else 0}")
        if len(markdown_content) > 20000:  # é’‰é’‰æ¶ˆæ¯é•¿åº¦é™åˆ¶
            markdown_content = markdown_content[:19700] + "\n\n...(å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶)"

        DingTalkRobot.send_markdown("Agent æ¨é€æŠ¥å‘Š", markdown_content)

        return markdown_content, file_path, result_store.get('ppt_path'), gr.update(value="Stop", interactive=True), gr.update(interactive=True)
    else:
        markdown_content, file_path = await deep_research(task=research_task,
                                                          llm=llm,
                                                          agent_state=_global_agent_state,
                                                          max_search_iterations=max_search_iteration_input,
                                                          max_query_num=max_query_per_iter_input,
                                                          use_vision=use_vision,
                                                          headless=headless,
                                                          use_own_browser=use_own_browser,
                                                          chrome_cdp=chrome_cdp
                                                          )

        # ç”ŸæˆPPTå¹¶è¿”å›æ–‡ä»¶è·¯å¾„
        ppt_path = None
        if file_path and os.path.exists(file_path) and markdown_content:
            try:
                # è°ƒç”¨get_pptå‡½æ•°ç”ŸæˆPPT
                ppt_path = get_ppt(markdown_content)
            except Exception as e:
                logger.error(f"ç”ŸæˆPPTå¤±è´¥: {str(e)}")
        else:
            # è®¡åˆ’æ¨¡å¼çš„ç»“æœå°†åœ¨å¼‚æ­¥å›è°ƒå‡½æ•°ä¸­å¤„ç†
            logger.info("è®¡åˆ’ä»»åŠ¡å·²åˆå§‹åŒ–ï¼Œç»“æœå°†åœ¨å¼‚æ­¥ä»»åŠ¡ä¸­å¤„ç†")

        if len(markdown_content) > 20000:  # é’‰é’‰æ¶ˆæ¯é•¿åº¦é™åˆ¶
            markdown_content = markdown_content[:19700] + "\n\n...(å†…å®¹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶)"

        DingTalkRobot.send_markdown("Agent æ¨é€æŠ¥å‘Š", markdown_content)

        return markdown_content, file_path, ppt_path, gr.update(value="Stop", interactive=True), gr.update(interactive=True)


def get_next_run_time():
    """
    è·å–ä¸‹ä¸€æ¬¡å®šæ—¶ä»»åŠ¡æ‰§è¡Œæ—¶é—´

    è¿”å›:
        datetime: ä¸‹ä¸€æ¬¡æ‰§è¡Œçš„æ—¶é—´ç‚¹ï¼Œå¦‚æœæ²¡æœ‰è°ƒåº¦ä»»åŠ¡åˆ™è¿”å›None
    """
    try:
        # è·å–å½“å‰è°ƒåº¦å™¨å®ä¾‹
        scheduler = app_state.get_scheduler()
        if not scheduler or not hasattr(scheduler, "running") or not scheduler.running:
            return None

        # è·å–æ‰€æœ‰ä»»åŠ¡
        jobs = scheduler.get_jobs()
        if not jobs:
            return None

        # æ‰¾å‡ºä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„ä»»åŠ¡æ—¶é—´
        next_run_time = None
        for job in jobs:
            if job.next_run_time:
                if next_run_time is None or job.next_run_time < next_run_time:
                    next_run_time = job.next_run_time

        return next_run_time
    except Exception as e:
        logger.error(f"è·å–ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´å¼‚å¸¸: {str(e)}")
        return None


def stop_scheduler():
    """
    åœæ­¢å®šæ—¶æ¨é€è°ƒåº¦å™¨

    è¿”å›:
        åœæ­¢ç»“æœæ¶ˆæ¯
    """
    try:
        logger.info("å°è¯•åœæ­¢è°ƒåº¦å™¨...")

        # è·å–å½“å‰è°ƒåº¦å™¨å®ä¾‹
        scheduler = app_state.get_scheduler()

        # åˆ¤æ–­è°ƒåº¦å™¨æ˜¯å¦å­˜åœ¨å¹¶ä¸”åœ¨è¿è¡Œ
        if scheduler and hasattr(scheduler, "running") and scheduler.running:
            logger.info("æ­£åœ¨å…³é—­è¿è¡Œä¸­çš„è°ƒåº¦å™¨...")

            # å°è¯•å…³é—­è°ƒåº¦å™¨
            try:
                scheduler.shutdown(wait=False)
                logger.info("è°ƒåº¦å™¨å·²æˆåŠŸå…³é—­")
            except Exception as shutdown_error:
                logger.warning(f"å…³é—­è°ƒåº¦å™¨æ—¶å‘ç”Ÿå¼‚å¸¸: {str(shutdown_error)}", exc_info=True)

            # é‡Šæ”¾è°ƒåº¦å™¨å®ä¾‹
            app_state.set_scheduler(None)

            # æ›´æ–°è°ƒåº¦å™¨çŠ¶æ€(éé˜»å¡æ¨¡å¼)
            scheduler_status = {
                "running": False,
                "next_run_time": None,
                "last_status": "è°ƒåº¦å™¨å·²åœæ­¢",
                "last_update_time": datetime.now()
            }
            app_state.update_scheduler_status(scheduler_status, blocking=False)
            logger.info("å·²è¯·æ±‚æ›´æ–°è°ƒåº¦å™¨çŠ¶æ€ä¸ºå·²åœæ­¢")

            success_msg = "è°ƒåº¦å™¨å·²æˆåŠŸåœæ­¢"
            logger.info(success_msg)
            return success_msg
        else:
            info_msg = "è°ƒåº¦å™¨å·²ç»å¤„äºåœæ­¢çŠ¶æ€"
            logger.info(info_msg)
            return info_msg

    except Exception as e:
        error_msg = f"åœæ­¢è°ƒåº¦å™¨æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        logger.error(error_msg, exc_info=True)

        # å°è¯•æ›´æ–°é”™è¯¯çŠ¶æ€
        try:
            app_state.update_scheduler_status({
                "running": False,
                "last_status": f"åœæ­¢å¤±è´¥: {str(e)[:50]}",
                "last_update_time": datetime.now(),
                "error": str(e)
            }, blocking=False)
            logger.info(f"å·²è¯·æ±‚æ›´æ–°è°ƒåº¦å™¨åœæ­¢å¤±è´¥çŠ¶æ€")
        except Exception as status_error:
            logger.error(f"æ— æ³•æ›´æ–°è°ƒåº¦å™¨çŠ¶æ€: {str(status_error)}", exc_info=True)

        return error_msg


def create_ui(theme_name="Ocean"):
    css = """
    .gradio-container {
        width: 60vw !important; 
        max-width: 60% !important; 
        margin-left: auto !important;
        margin-right: auto !important;
        padding-top: 20px !important;
    }
    .header-text {
        text-align: center;
        margin-bottom: 30px;
    }
    .theme-section {
        margin-bottom: 20px;
        padding: 15px;
        border-radius: 10px;
    }
    """

    with gr.Blocks(
            title="Browser Use WebUI", theme=theme_map[theme_name], css=css
    ) as demo:
        with gr.Row():
            gr.Markdown(
                """
                # ğŸŒ Browser Use WebUI
                ### Control your browser with AI assistance
                """,
                elem_classes=["header-text"],
            )

        with gr.Tabs() as tabs:
            with gr.TabItem("âš™ï¸ Agent Settings", id=1):
                with gr.Group():
                    agent_type = gr.Radio(
                        ["org", "custom"],
                        label="Agent Type",
                        value="custom",
                        info="Select the type of agent to use",
                        interactive=True
                    )
                    with gr.Column():
                        max_steps = gr.Slider(
                            minimum=1,
                            maximum=200,
                            value=100,
                            step=1,
                            label="Max Run Steps",
                            info="Maximum number of steps the agent will take",
                            interactive=True
                        )
                        max_actions_per_step = gr.Slider(
                            minimum=1,
                            maximum=100,
                            value=10,
                            step=1,
                            label="Max Actions per Step",
                            info="Maximum number of actions the agent will take per step",
                            interactive=True
                        )
                    with gr.Column():
                        use_vision = gr.Checkbox(
                            label="Use Vision",
                            value=True,
                            info="Enable visual processing capabilities",
                            interactive=True
                        )
                        max_input_tokens = gr.Number(
                            label="Max Input Tokens",
                            value=5120000,
                            precision=0,
                            interactive=True
                        )
                        tool_calling_method = gr.Dropdown(
                            label="Tool Calling Method",
                            value="auto",
                            interactive=True,
                            allow_custom_value=True,  # Allow users to input custom model names
                            choices=["auto", "json_schema", "function_calling"],
                            info="Tool Calls Funtion Name",
                            visible=False
                        )

            with gr.TabItem("ğŸ”§ LLM Settings", id=2):
                with gr.Group():
                    llm_provider = gr.Dropdown(
                        choices=[provider for provider, model in utils.model_names.items()],
                        label="LLM Provider",
                        value="alibaba",
                        info="Select your preferred language model provider",
                        interactive=True
                    )
                    llm_model_name = gr.Dropdown(
                        label="Model Name",
                        choices=utils.model_names['alibaba'],
                        value="qwen-max",
                        interactive=True,
                        allow_custom_value=True,  # Allow users to input custom model names
                        info="Select a model in the dropdown options or directly type a custom model name"
                    )
                    ollama_num_ctx = gr.Slider(
                        minimum=2 ** 8,
                        maximum=2 ** 16,
                        value=16000,
                        step=1,
                        label="Ollama Context Length",
                        info="Controls max context length model needs to handle (less = faster)",
                        visible=False,
                        interactive=True
                    )
                    llm_temperature = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.2,
                        step=0.1,
                        label="Temperature",
                        info="Controls randomness in model outputs",
                        interactive=True
                    )
                    with gr.Row():
                        llm_base_url = gr.Textbox(
                            label="Base URL",
                            value="",
                            info="API endpoint URL (if required)"
                        )
                        llm_api_key = gr.Textbox(
                            label="API Key",
                            type="password",
                            value="",
                            info="Your API key (leave blank to use .env)"
                        )

            # Change event to update context length slider
            def update_llm_num_ctx_visibility(llm_provider):
                return gr.update(visible=llm_provider == "ollama")

            # Bind the change event of llm_provider to update the visibility of context length slider
            llm_provider.change(
                fn=update_llm_num_ctx_visibility,
                inputs=llm_provider,
                outputs=ollama_num_ctx
            )

            with gr.TabItem("ğŸŒ Browser Settings", id=3):
                with gr.Group():
                    with gr.Row():
                        use_own_browser = gr.Checkbox(
                            label="Use Own Browser",
                            value=True,
                            info="Use your existing browser instance",
                            interactive=True
                        )
                        keep_browser_open = gr.Checkbox(
                            label="Keep Browser Open",
                            value=False,
                            info="Keep Browser Open between Tasks",
                            interactive=True
                        )
                        headless = gr.Checkbox(
                            label="Headless Mode",
                            value=False,
                            info="Run browser without GUI",
                            interactive=True
                        )
                        disable_security = gr.Checkbox(
                            label="Disable Security",
                            value=True,
                            info="Disable browser security features",
                            interactive=True
                        )
                        enable_recording = gr.Checkbox(
                            label="Enable Recording",
                            value=True,
                            info="Enable saving browser recordings",
                            interactive=True
                        )

                    with gr.Row():
                        window_w = gr.Number(
                            label="Window Width",
                            value=1280,
                            info="Browser window width",
                            interactive=True
                        )
                        window_h = gr.Number(
                            label="Window Height",
                            value=1100,
                            info="Browser window height",
                            interactive=True
                        )

                    chrome_cdp = gr.Textbox(
                        label="CDP URL",
                        placeholder="http://localhost:9222",
                        value="",
                        info="CDP for google remote debugging",
                        interactive=True,  # Allow editing only if recording is enabled
                    )

                    save_recording_path = gr.Textbox(
                        label="Recording Path",
                        placeholder="e.g. ./tmp/record_videos",
                        value="./tmp/record_videos",
                        info="Path to save browser recordings",
                        interactive=True,  # Allow editing only if recording is enabled
                    )

                    save_trace_path = gr.Textbox(
                        label="Trace Path",
                        placeholder="e.g. ./tmp/traces",
                        value="./tmp/traces",
                        info="Path to save Agent traces",
                        interactive=True,
                    )

                    save_agent_history_path = gr.Textbox(
                        label="Agent History Save Path",
                        placeholder="e.g., ./tmp/agent_history",
                        value="./tmp/agent_history",
                        info="Specify the directory where agent history should be saved.",
                        interactive=True,
                    )

            with gr.TabItem("ğŸ¤– Run Agent", id=4):
                task = gr.Textbox(
                    label="Task Description",
                    lines=4,
                    placeholder="Enter your task here...",
                    value="go to google.com and type 'OpenAI' click search and give me the first url",
                    info="Describe what you want the agent to do",
                    interactive=True
                )
                add_infos = gr.Textbox(
                    label="Additional Information",
                    lines=3,
                    placeholder="Add any helpful context or instructions...",
                    info="Optional hints to help the LLM complete the task",
                    value="",
                    interactive=True
                )

                with gr.Row():
                    run_button = gr.Button("â–¶ï¸ Run Agent", variant="primary", scale=2)
                    stop_button = gr.Button("â¹ï¸ Stop", variant="stop", scale=1)

                with gr.Row():
                    browser_view = gr.HTML(
                        value="<h1 style='width:80vw; height:50vh'>Waiting for browser session...</h1>",
                        label="Live Browser View",
                        visible=False
                    )

                gr.Markdown("### Results")
                with gr.Row():
                    with gr.Column():
                        final_result_output = gr.Textbox(
                            label="Final Result", lines=3, show_label=True
                        )
                    with gr.Column():
                        errors_output = gr.Textbox(
                            label="Errors", lines=3, show_label=True
                        )
                with gr.Row():
                    with gr.Column():
                        model_actions_output = gr.Textbox(
                            label="Model Actions", lines=3, show_label=True, visible=False
                        )
                    with gr.Column():
                        model_thoughts_output = gr.Textbox(
                            label="Model Thoughts", lines=3, show_label=True, visible=False
                        )
                recording_gif = gr.Image(label="Result GIF", format="gif")
                trace_file = gr.File(label="Trace File")
                agent_history_file = gr.File(label="Agent History")

            with gr.TabItem("ğŸ§ Deep Research", id=5):
                research_task_input = gr.Textbox(label="Research Task", lines=5,
                                                 value="""è®¿é—®æŠ–éŸ³åº—é“ºåå°ï¼šhttps://compass.jinritemai.com/shopæŸ¥çœ‹ç›¸å…³æ•°æ® 
ç„¶åè®¿é—®åº—é“ºæ¦œå•ã€å•†å“æ¦œå•ã€è¡Œä¸šæœç´¢è¯æ¦œå•ã€çœ‹åæœç´¢è¯æ¦œå•ï¼Œé“¾æ¥åˆ†åˆ«ä¸ºï¼š
https://compass.jinritemai.com/shop/chance/search-shop-rank?rank_type=2&from_page=%2Fshop%2Fchance%2Fsearch-word-rank&from=sy&btm_ppre=a6187.b01487.c0.d0&btm_pre=a6187.b17762.c0.d0&btm_show_id=a1969687-d888-471f-b4ea-070065bd18d9

https://compass.jinritemai.com/shop/chance/search-rank-product?rank_type=1&from_page=%2Fshop%2Fchance%2Fsearch-shop-rank&from=sy&btm_ppre=a6187.b17762.c0.d0&btm_pre=a6187.b857760.c0.d0&btm_show_id=e3a96330-bf9f-42e7-a527-9f274c00187c

https://compass.jinritemai.com/shop/chance/search-word-rank?from_page=%2Fshop%2Fchance%2Fsearch-rank-product&from=sy&btm_ppre=a6187.b857760.c0.d0&btm_pre=a6187.b779894.c0.d0&btm_show_id=5075dd62-5e48-499f-9ad6-efd6f15ef4a8

https://compass.jinritemai.com/shop/chance/search-product-rank?rank_type=4&from_page=%2Fshop%2Fchance%2Fsearch-word-rank&from=sy&btm_ppre=a6187.b779894.c0.d0&btm_pre=a6187.b17762.c0.d0&btm_show_id=15a427c1-e6b9-4d19-9b25-cda24a9fb053
ç»“åˆé¡µé¢ä¸Šçš„å¤šä¸ªæ’è¡Œæ¦œé¦–é¡µçš„ç›¸å…³æ•°æ®ï¼Œç»“åˆè‡ªå·±åº—é“ºçš„è¡Œä¸šç±»ç›®ï¼ŒæŸ¥è¯¢ç›¸å…³çš„è¡Œä¸šæ–°é—»ï¼Œå…·ä½“å‚è€ƒä¸‹åˆ—ç½‘ç«™ï¼š
https://www.dsb.cn/  ç”µå•†æ´¾

https://www.ebrun.com/ äº¿é‚¦åŠ¨åŠ›

https://www.cifnews.com/ é›¨æœè·¨å¢ƒ

ç»“åˆä¸Šé¢æ–°é—»ç½‘ç«™æœ€æ–°å¯èƒ½ç›¸å…³çš„æ–°é—»ï¼Œç”Ÿæˆç›¸å…³ç±»ç›®çš„è¡Œä¸šåˆ†ææŠ¥å‘Šï¼Œç»“åˆç”¨æˆ·çš„æ•°æ®ç»™ç”¨æˆ·æå‡ºä¸€äº›å»ºè®®ï¼Œä»¥åŠäº†è§£å¸‚åœºçš„æ–¹å‘è¶‹åŠ¿ã€‚""",
                                                 interactive=True)
                # æ·»åŠ å®šæ—¶æ¨é€è®¾ç½®åŒºåŸŸ
                with gr.Accordion("â° å®šæ—¶æ¨é€è®¾ç½®", open=False):
                    with gr.Row():
                        enable_schedule = gr.Checkbox(
                            label="å¯ç”¨å®šæ—¶æ¨é€",
                            value=False,
                            info="å¯ç”¨åå°†æŒ‰è®¾å®šé—´éš”è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡",
                            interactive=True
                        )
                    with gr.Row():
                        with gr.Column():
                            schedule_interval = gr.Slider(
                                label="æ¨é€é—´éš”ï¼ˆå°æ—¶ï¼‰",
                                minimum=0.1,
                                maximum=24,
                                step=0.1,
                                value=1,
                                info="ä»»åŠ¡æ‰§è¡Œé—´éš”æ—¶é—´ï¼Œå°æ•°ç‚¹è¡¨ç¤ºå°æ—¶åˆ†æ•°",
                                interactive=True
                            )
                        with gr.Column():
                            schedule_title = gr.Textbox(
                                label="æ¨é€æ ‡é¢˜å‰ç¼€",
                                value="æ·±åº¦æœç´¢",
                                info="æ¶ˆæ¯æ ‡é¢˜å‰ç¼€ï¼Œå°†ä¸ä»»åŠ¡å†…å®¹æ‹¼æ¥",
                                interactive=True
                            )

                with gr.Row():
                    max_search_iteration_input = gr.Number(label="Max Search Iteration", value=3,
                                                           precision=0,
                                                           interactive=True)  # precision=0 ç¡®ä¿æ˜¯æ•´æ•°
                    max_query_per_iter_input = gr.Number(label="Max Query per Iteration", value=1,
                                                         precision=0,
                                                         interactive=True)  # precision=0 ç¡®ä¿æ˜¯æ•´æ•°
                with gr.Row():
                    research_button = gr.Button("â–¶ï¸ Run Deep Research", variant="primary", scale=2)
                    stop_research_button = gr.Button("â¹ Stop", variant="stop", scale=1)
                    task_opt_button = gr.Button("æç¤ºè¯ç¾åŒ–", variant="stop", scale=1)

                markdown_output_display = gr.Markdown(label="Research Report")
                schedule_result_text = gr.Markdown("å®šæ—¶ä»»åŠ¡çŠ¶æ€å°†åœ¨æ­¤æ˜¾ç¤º...")
                # Markdownå’ŒPPTä¸‹è½½éƒ¨åˆ†
                with gr.Row():
                    markdown_download = gr.File(label="ä¸‹è½½MarkdownæŠ¥å‘Š", interactive=False)
                with gr.Row():
                    ppt_download = gr.File(label="ä¸‹è½½PPTæŠ¥å‘Š", interactive=False)

            # Bind the stop button click event after errors_output is defined
            stop_button.click(
                fn=stop_agent,
                inputs=[],
                outputs=[stop_button, run_button],
            )

            # Run button click handler
            run_button.click(
                fn=run_with_stream,
                inputs=[
                    agent_type, llm_provider, llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url,
                    llm_api_key,
                    use_own_browser, keep_browser_open, headless, disable_security, window_w, window_h,
                    save_recording_path, save_agent_history_path, save_trace_path,  # Include the new path
                    enable_recording, task, add_infos, max_steps, use_vision, max_actions_per_step,
                    tool_calling_method, chrome_cdp, max_input_tokens
                ],
                outputs=[
                    browser_view,  # Browser view
                    final_result_output,  # Final result
                    errors_output,  # Errors
                    model_actions_output,  # Model actions
                    model_thoughts_output,  # Model thoughts
                    recording_gif,  # Latest recording
                    trace_file,  # Trace file
                    agent_history_file,  # Agent history file
                    stop_button,  # Stop button
                    run_button  # Run button
                ],
            )

            # ç‚¹å‡»æŒ‰é’®ç”Ÿæˆå¹¶ä¸‹è½½PPTçš„å‡½æ•°
            def generate_ppt_on_click(markdown_content):
                if not markdown_content:
                    return None
                try:
                    from src.utils.ppt_download import get_ppt
                    ppt_path = get_ppt(markdown_content)
                    logger.info(f"Generated PPT at: {ppt_path}")
                    # è¿”å›æ–‡ä»¶è·¯å¾„å’Œæ›´æ–°æ˜¾ç¤ºçŠ¶æ€
                    return ppt_path, gr.update(visible=True)
                except Exception as e:
                    logger.error(f"Error generating PPT: {str(e)}")
                    return None, gr.update(visible=False)

            # Run Deep Research
            research_button.click(
                fn=run_deep_search,
                inputs=[
                    enable_schedule,
                    schedule_interval,
                    schedule_title,
                    research_task_input, max_search_iteration_input, max_query_per_iter_input, llm_provider,
                    llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key, use_vision,
                    use_own_browser, headless, chrome_cdp],
                outputs=[markdown_output_display, markdown_download, ppt_download, stop_research_button,
                         research_button]
            )

            # Bind the stop button click event
            stop_research_button.click(
                fn=stop_research_agent,
                inputs=[],
                outputs=[stop_research_button, research_button],
            )

            # Connect the task_opt_button to the optimize_prompt function
            task_opt_button.click(
                fn=optimize_prompt,
                inputs=[research_task_input, llm_provider,
                        llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key],
                outputs=[research_task_input]
            )

            # æ·»åŠ å®šæ—¶æ¨é€è®¾ç½®æ ‡ç­¾é¡µ
            (
                scheduler_enabled, scheduler_interval, scheduler_task, scheduler_title,
                scheduler_iterations, scheduler_queries, scheduler_status_text,
                scheduler_next_run_text, scheduler_start_btn, scheduler_stop_btn,
                scheduler_run_once_btn, scheduler_result_text
            ) = create_scheduler_tab()

            # ç»‘å®šå®šæ—¶æ¨é€æ ‡ç­¾é¡µçš„äº‹ä»¶å¤„ç†
            # åˆ›å»ºä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥è½¬æ¢å°æ—¶åˆ°åˆ†é’Ÿ
            def convert_hours_to_minutes(llm_provider,
                                         llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key,
                                         hours, task, title, iterations, queries):
                minutes = int(float(hours) * 60)  # å°†å°æ—¶è½¬æ¢ä¸ºåˆ†é’Ÿ
                return start_scheduler(llm_provider,
                                       llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key, task,
                                       minutes, title, iterations, queries)

            scheduler_start_btn.click(
                fn=convert_hours_to_minutes,
                inputs=[
                    llm_provider,
                    llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key,
                    scheduler_interval,
                    scheduler_task,
                    scheduler_title,
                    scheduler_iterations,
                    scheduler_queries
                ],
                outputs=[scheduler_result_text]
            )

            scheduler_stop_btn.click(
                fn=stop_scheduler,
                inputs=[],
                outputs=[scheduler_result_text, scheduler_status_text, scheduler_next_run_text]
            )

            scheduler_run_once_btn.click(
                fn=run_push_task_once,
                inputs=[
                    llm_provider,
                    llm_model_name, ollama_num_ctx, llm_temperature, llm_base_url, llm_api_key,
                    scheduler_task,
                    scheduler_title,
                    scheduler_iterations,
                    scheduler_queries
                ],
                outputs=[scheduler_result_text, scheduler_status_text, scheduler_next_run_text]
            )
            with gr.TabItem("ğŸ¥ Recordings", id=7, visible=True):
                def list_recordings(save_recording_path):
                    if not os.path.exists(save_recording_path):
                        return []

                    # Get all video files
                    recordings = glob.glob(os.path.join(save_recording_path, "*.[mM][pP]4")) + glob.glob(
                        os.path.join(save_recording_path, "*.[wW][eE][bB][mM]"))

                    # Sort recordings by creation time (oldest first)
                    recordings.sort(key=os.path.getctime)

                    # Add numbering to the recordings
                    numbered_recordings = []
                    for idx, recording in enumerate(recordings, start=1):
                        filename = os.path.basename(recording)
                        numbered_recordings.append((recording, f"{idx}. {filename}"))

                    return numbered_recordings

                recordings_gallery = gr.Gallery(
                    label="Recordings",
                    columns=3,
                    height="auto",
                    object_fit="contain"
                )

                refresh_button = gr.Button("ğŸ”„ Refresh Recordings", variant="secondary")
                refresh_button.click(
                    fn=list_recordings,
                    inputs=save_recording_path,
                    outputs=recordings_gallery
                )

            with gr.TabItem("ğŸ“ UI Configuration", id=8):
                config_file_input = gr.File(
                    label="Load UI Settings from Config File",
                    file_types=[".json"],
                    interactive=True
                )
                with gr.Row():
                    load_config_button = gr.Button("Load Config", variant="primary")
                    save_config_button = gr.Button("Save UI Settings", variant="primary")

                config_status = gr.Textbox(
                    label="Status",
                    lines=2,
                    interactive=False
                )
                save_config_button.click(
                    fn=save_current_config,
                    inputs=[],  # ä¸éœ€è¦è¾“å…¥å‚æ•°
                    outputs=[config_status]
                )

        # Attach the callback to the LLM provider dropdown
        llm_provider.change(
            lambda provider, api_key, base_url: update_model_dropdown(provider, api_key, base_url),
            inputs=[llm_provider, llm_api_key, llm_base_url],
            outputs=llm_model_name
        )

        # Add this after defining the components
        enable_recording.change(
            lambda enabled: gr.update(interactive=enabled),
            inputs=enable_recording,
            outputs=save_recording_path
        )

        use_own_browser.change(fn=close_global_browser)
        keep_browser_open.change(fn=close_global_browser)

        scan_and_register_components(demo)
        global webui_config_manager
        all_components = webui_config_manager.get_all_components()

        load_config_button.click(
            fn=update_ui_from_config,
            inputs=[config_file_input],
            outputs=all_components + [config_status]
        )
    return demo


def main():
    parser = argparse.ArgumentParser(description="Gradio UI for Browser Agent")
    parser.add_argument("--ip", type=str, default="127.0.0.1", help="IP address to bind to")
    parser.add_argument("--port", type=int, default=7788, help="Port to listen on")
    parser.add_argument("--theme", type=str, default="Ocean", choices=theme_map.keys(), help="Theme to use for the UI")
    args = parser.parse_args()

    demo = create_ui(theme_name=args.theme)
    demo.launch(server_name=args.ip, server_port=args.port)


if __name__ == '__main__':
    main()
